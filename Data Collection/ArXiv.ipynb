{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries. \n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we import all the necessary libraries for fetching data from the ArXiv API, processing XML responses, cleaning LaTeX-formatted text, and managing CSV files. The libraries include:\n",
    "- `requests` for sending HTTP requests to fetch data.\n",
    "- `xml.etree.ElementTree` for parsing the XML response from the ArXiv API.\n",
    "- `csv` for saving the filtered papers into a CSV file.\n",
    "- `re` for regular expression-based text processing to clean LaTeX formatting.\n",
    "- `pandas` for managing and reading CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the file\n",
    "\n",
    "def clean_latex(text):\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)  # Remove inline math\n",
    "    text = re.sub(r'\\\\begin{.*?}.*?\\\\end{.*?}', '', text, flags=re.DOTALL)  # Remove math environments\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+{.*?}', '', text)  # Remove commands with arguments\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\s*', '', text)  # Remove standalone commands\n",
    "    text = text.replace('{', '').replace('}', '')  # Remove curly braces\n",
    "    text = re.sub(r'\\\\cite{.*?}', '', text)  # Remove citations\n",
    "    text = re.sub(r'(Figure|Table) \\d+', '', text)  # Remove figure/table references\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the LaTeX Formatted Text\n",
    "\n",
    "This function `clean_latex()` removes LaTeX formatting from the summary of each paper. It specifically targets:\n",
    "- Inline math expressions and LaTeX environments (e.g., equations).\n",
    "- LaTeX commands such as `\\textbf{}` or `\\section{}`.\n",
    "- Citations, figure/table references, and other irrelevant technical terms.\n",
    "\n",
    "The goal is to make the text more readable and remove any non-relevant LaTeX syntax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ArXiv API endpoint and query parameters\n",
    "# https://arxiv.org/category_taxonomy Link \n",
    "\n",
    "url = \"http://export.arxiv.org/api/query\"\n",
    "params = {\n",
    "    'search_query': 'cat:cs.LG',\n",
    "    'start': 0,\n",
    "    'max_results': 230,  # Fetch 230 papers\n",
    "    'sortBy': 'relevance',\n",
    "    'sortOrder': 'descending'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up the ArXiv API Query\n",
    "\n",
    "Here, we define the API endpoint and the query parameters. The `params` dictionary contains:\n",
    "- `search_query`: Filtering papers in the `cs.LG` (Machine Learning) category.\n",
    "- `max_results`: Fetching 230 papers to ensure that after filtering by keywords, we end up with a sufficient number of relevant papers (20 papers in this case).\n",
    "- `sortBy` and `sortOrder`: Sorting papers by relevance in descending order.\n",
    "\n",
    "This ensures we only fetch the most relevant papers related to machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send request to ArXiv API\n",
    "\n",
    "response = requests.get(url, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sending the Request to the ArXiv API\n",
    "\n",
    "We use the `requests.get()` method to send a request to the ArXiv API with the defined query parameters. The response is an XML document containing metadata about the papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the response\n",
    "\n",
    "def parse_papers(root):\n",
    "    papers = []\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
    "        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()\n",
    "        link = entry.find('{http://www.w3.org/2005/Atom}id').text.strip()\n",
    "        papers.append({'title': title, 'summary': summary, 'link': link})\n",
    "    return papers\n",
    "\n",
    "root = ET.fromstring(response.content)\n",
    "papers = parse_papers(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the Response\n",
    "\n",
    "This function, `parse_papers()`, processes the XML response to extract the title, summary, and link of each paper. The function loops through the entries in the XML and stores the relevant information in a list of dictionaries for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'model selection', 'cross-validation', 'hyperparameter tuning', 'grid search',\n",
    "    'Bayesian optimization', 'train-test split', 'performance metrics', 'k-fold cross-validation',\n",
    "    'leave-one-out cross-validation', 'regularization', 'L1 regularization', 'L2 regularization',\n",
    "    'AUC-ROC', 'hyperparameter optimization', 'early stopping', 'overfitting prevention',\n",
    "    'bias-variance tradeoff', 'dropout', 'weight decay'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Filtering Keywords\n",
    "\n",
    "Here, we define a list of keywords related to model selection and machine learning techniques. These keywords are used to filter the paper's summaries and retain only the ones that discuss topics such as:\n",
    "- Model selection techniques (e.g., cross-validation, hyperparameter tuning).\n",
    "- Regularization methods and optimization strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter papers by keywords\n",
    "\n",
    "def filter_papers_by_keywords(papers, keywords):\n",
    "    filtered_papers = [paper for paper in papers if any(kw in paper['summary'].lower() for kw in keywords)]\n",
    "    return filtered_papers\n",
    "\n",
    "filtered_papers = filter_papers_by_keywords(papers, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Papers by Keywords\n",
    "\n",
    "The function `filter_papers_by_keywords()` filters the papers by checking if any of the keywords from the list appear in the summary. This ensures that only the papers discussing relevant machine learning topics are retained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in filtered_papers:\n",
    "    paper['cleaned_summary'] = clean_latex(paper['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Summaries of Filtered Papers\n",
    "\n",
    "For each of the filtered papers, we clean the summary using the `clean_latex()` function to remove any LaTeX formatting and improve readability. This helps make the summaries more accessible and easier to understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Paper 1: Efficient algorithms for decision tree cross-validation\n",
      "\n",
      "Link: http://arxiv.org/abs/cs/0110036v1\n",
      "\n",
      "Original Summary: Cross-validation is a useful and generally applicable technique often\n",
      "employed in machine learning, including decision tree induction. An important\n",
      "disadvantage of straightforward implementation of the technique is its\n",
      "computational overhead. In this paper we show that, for decision trees, the\n",
      "computational overhead of cross-validation can be reduced significantly by\n",
      "integrating the cross-validation with the normal decision tree induction\n",
      "process. We discuss how existing decision tree algorithms can be adapted to\n",
      "this aim, and provide an analysis of the speedups these adaptations may yield.\n",
      "The analysis is supported by experimental results.\n",
      "\n",
      "Cleaned Summary: Cross-validation is a useful and generally applicable technique often\n",
      "employed in machine learning, including decision tree induction. An important\n",
      "disadvantage of straightforward implementation of the technique is its\n",
      "computational overhead. In this paper we show that, for decision trees, the\n",
      "computational overhead of cross-validation can be reduced significantly by\n",
      "integrating the cross-validation with the normal decision tree induction\n",
      "process. We discuss how existing decision tree algorithms can be adapted to\n",
      "this aim, and provide an analysis of the speedups these adaptations may yield.\n",
      "The analysis is supported by experimental results.\n",
      "\n",
      "Filtered Paper 2: Stability Analysis for Regularized Least Squares Regression\n",
      "\n",
      "Link: http://arxiv.org/abs/cs/0502016v1\n",
      "\n",
      "Original Summary: We discuss stability for a class of learning algorithms with respect to noisy\n",
      "labels. The algorithms we consider are for regression, and they involve the\n",
      "minimization of regularized risk functionals, such as L(f) := 1/N sum_i\n",
      "(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when\n",
      "y_i is a noisy version of f*(x_i) for some function f* in H, the output of the\n",
      "algorithm converges to f* as the regularization term and noise simultaneously\n",
      "vanish. We consider two flavors of this problem, one where a data set of N\n",
      "points remains fixed, and the other where N -> infinity. For the case where N\n",
      "-> infinity, we give conditions for convergence to f_E (the function which is\n",
      "the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we\n",
      "describe the limiting 'non-noisy', 'non-regularized' function f*, and give\n",
      "conditions for convergence. In the process, we develop a set of tools for\n",
      "dealing with functionals such as L(f), which are applicable to many other\n",
      "problems in learning theory.\n",
      "\n",
      "Cleaned Summary: We discuss stability for a class of learning algorithms with respect to noisy\n",
      "labels. The algorithms we consider are for regression, and they involve the\n",
      "minimization of regularized risk functionals, such as L(f) := 1/N sum_i\n",
      "(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when\n",
      "y_i is a noisy version of f*(x_i) for some function f* in H, the output of the\n",
      "algorithm converges to f* as the regularization term and noise simultaneously\n",
      "vanish. We consider two flavors of this problem, one where a data set of N\n",
      "points remains fixed, and the other where N -> infinity. For the case where N\n",
      "-> infinity, we give conditions for convergence to f_E (the function which is\n",
      "the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we\n",
      "describe the limiting 'non-noisy', 'non-regularized' function f*, and give\n",
      "conditions for convergence. In the process, we develop a set of tools for\n",
      "dealing with functionals such as L(f), which are applicable to many other\n",
      "problems in learning theory.\n",
      "\n",
      "Filtered Paper 3: Parametric Learning and Monte Carlo Optimization\n",
      "\n",
      "Link: http://arxiv.org/abs/0704.1274v1\n",
      "\n",
      "Original Summary: This paper uncovers and explores the close relationship between Monte Carlo\n",
      "Optimization of a parametrized integral (MCO), Parametric machine-Learning\n",
      "(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\n",
      "contributions. First, we prove that MCO is mathematically identical to a broad\n",
      "class of PL problems. This identity potentially provides a new application\n",
      "domain for all broadly applicable PL techniques: MCO. Second, we introduce\n",
      "immediate sampling, a new version of the Probability Collectives (PC) algorithm\n",
      "for blackbox optimization. Immediate sampling transforms the original BO\n",
      "problem into an MCO problem. Accordingly, by combining these first two\n",
      "contributions, we can apply all PL techniques to BO. In our third contribution\n",
      "we validate this way of improving BO by demonstrating that cross-validation and\n",
      "bagging improve immediate sampling. Finally, conventional MC and MCO procedures\n",
      "ignore the relationship between the sample point locations and the associated\n",
      "values of the integrand; only the values of the integrand at those locations\n",
      "are considered. We demonstrate that one can exploit the sample location\n",
      "information using PL techniques, for example by forming a fit of the sample\n",
      "locations to the associated values of the integrand. This provides an\n",
      "additional way to apply PL techniques to improve MCO.\n",
      "\n",
      "Cleaned Summary: This paper uncovers and explores the close relationship between Monte Carlo\n",
      "Optimization of a parametrized integral (MCO), Parametric machine-Learning\n",
      "(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\n",
      "contributions. First, we prove that MCO is mathematically identical to a broad\n",
      "class of PL problems. This identity potentially provides a new application\n",
      "domain for all broadly applicable PL techniques: MCO. Second, we introduce\n",
      "immediate sampling, a new version of the Probability Collectives (PC) algorithm\n",
      "for blackbox optimization. Immediate sampling transforms the original BO\n",
      "problem into an MCO problem. Accordingly, by combining these first two\n",
      "contributions, we can apply all PL techniques to BO. In our third contribution\n",
      "we validate this way of improving BO by demonstrating that cross-validation and\n",
      "bagging improve immediate sampling. Finally, conventional MC and MCO procedures\n",
      "ignore the relationship between the sample point locations and the associated\n",
      "values of the integrand; only the values of the integrand at those locations\n",
      "are considered. We demonstrate that one can exploit the sample location\n",
      "information using PL techniques, for example by forming a fit of the sample\n",
      "locations to the associated values of the integrand. This provides an\n",
      "additional way to apply PL techniques to improve MCO.\n",
      "\n",
      "Filtered Paper 4: Consistency of the group Lasso and multiple kernel learning\n",
      "\n",
      "Link: http://arxiv.org/abs/0707.3390v2\n",
      "\n",
      "Original Summary: We consider the least-square regression problem with regularization by a\n",
      "block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\n",
      "than one. This problem, referred to as the group Lasso, extends the usual\n",
      "regularization by the 1-norm where all spaces have dimension one, where it is\n",
      "commonly referred to as the Lasso. In this paper, we study the asymptotic model\n",
      "consistency of the group Lasso. We derive necessary and sufficient conditions\n",
      "for the consistency of group Lasso under practical assumptions, such as model\n",
      "misspecification. When the linear predictors and Euclidean norms are replaced\n",
      "by functions and reproducing kernel Hilbert norms, the problem is usually\n",
      "referred to as multiple kernel learning and is commonly used for learning from\n",
      "heterogeneous data sources and for non linear variable selection. Using tools\n",
      "from functional analysis, and in particular covariance operators, we extend the\n",
      "consistency results to this infinite dimensional case and also propose an\n",
      "adaptive scheme to obtain a consistent model estimate, even when the necessary\n",
      "condition required for the non adaptive scheme is not satisfied.\n",
      "\n",
      "Cleaned Summary: We consider the least-square regression problem with regularization by a\n",
      "block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\n",
      "than one. This problem, referred to as the group Lasso, extends the usual\n",
      "regularization by the 1-norm where all spaces have dimension one, where it is\n",
      "commonly referred to as the Lasso. In this paper, we study the asymptotic model\n",
      "consistency of the group Lasso. We derive necessary and sufficient conditions\n",
      "for the consistency of group Lasso under practical assumptions, such as model\n",
      "misspecification. When the linear predictors and Euclidean norms are replaced\n",
      "by functions and reproducing kernel Hilbert norms, the problem is usually\n",
      "referred to as multiple kernel learning and is commonly used for learning from\n",
      "heterogeneous data sources and for non linear variable selection. Using tools\n",
      "from functional analysis, and in particular covariance operators, we extend the\n",
      "consistency results to this infinite dimensional case and also propose an\n",
      "adaptive scheme to obtain a consistent model estimate, even when the necessary\n",
      "condition required for the non adaptive scheme is not satisfied.\n",
      "\n",
      "Filtered Paper 5: Consistency of trace norm minimization\n",
      "\n",
      "Link: http://arxiv.org/abs/0710.2848v1\n",
      "\n",
      "Original Summary: Regularization by the sum of singular values, also referred to as the trace\n",
      "norm, is a popular technique for estimating low rank rectangular matrices. In\n",
      "this paper, we extend some of the consistency results of the Lasso to provide\n",
      "necessary and sufficient conditions for rank consistency of trace norm\n",
      "minimization with the square loss. We also provide an adaptive version that is\n",
      "rank consistent even when the necessary condition for the non adaptive version\n",
      "is not fulfilled.\n",
      "\n",
      "Cleaned Summary: Regularization by the sum of singular values, also referred to as the trace\n",
      "norm, is a popular technique for estimating low rank rectangular matrices. In\n",
      "this paper, we extend some of the consistency results of the Lasso to provide\n",
      "necessary and sufficient conditions for rank consistency of trace norm\n",
      "minimization with the square loss. We also provide an adaptive version that is\n",
      "rank consistent even when the necessary condition for the non adaptive version\n",
      "is not fulfilled.\n",
      "\n",
      "Filtered Paper 6: A New Approach to Collaborative Filtering: Operator Estimation with\n",
      "  Spectral Regularization\n",
      "\n",
      "Link: http://arxiv.org/abs/0802.1430v2\n",
      "\n",
      "Original Summary: We present a general approach for collaborative filtering (CF) using spectral\n",
      "regularization to learn linear operators from \"users\" to the \"objects\" they\n",
      "rate. Recent low-rank type matrix completion approaches to CF are shown to be\n",
      "special cases. However, unlike existing regularization based CF methods, our\n",
      "approach can be used to also incorporate information such as attributes of the\n",
      "users or the objects -- a limitation of existing regularization based CF\n",
      "methods. We then provide novel representer theorems that we use to develop new\n",
      "estimation methods. We provide learning algorithms based on low-rank\n",
      "decompositions, and test them on a standard CF dataset. The experiments\n",
      "indicate the advantages of generalizing the existing regularization based CF\n",
      "methods to incorporate related information about users and objects. Finally, we\n",
      "show that certain multi-task learning methods can be also seen as special cases\n",
      "of our proposed approach.\n",
      "\n",
      "Cleaned Summary: We present a general approach for collaborative filtering (CF) using spectral\n",
      "regularization to learn linear operators from \"users\" to the \"objects\" they\n",
      "rate. Recent low-rank type matrix completion approaches to CF are shown to be\n",
      "special cases. However, unlike existing regularization based CF methods, our\n",
      "approach can be used to also incorporate information such as attributes of the\n",
      "users or the objects -- a limitation of existing regularization based CF\n",
      "methods. We then provide novel representer theorems that we use to develop new\n",
      "estimation methods. We provide learning algorithms based on low-rank\n",
      "decompositions, and test them on a standard CF dataset. The experiments\n",
      "indicate the advantages of generalizing the existing regularization based CF\n",
      "methods to incorporate related information about users and objects. Finally, we\n",
      "show that certain multi-task learning methods can be also seen as special cases\n",
      "of our proposed approach.\n",
      "\n",
      "Filtered Paper 7: A Quadratic Loss Multi-Class SVM\n",
      "\n",
      "Link: http://arxiv.org/abs/0804.4898v1\n",
      "\n",
      "Original Summary: Using a support vector machine requires to set two types of hyperparameters:\n",
      "the soft margin parameter C and the parameters of the kernel. To perform this\n",
      "model selection task, the method of choice is cross-validation. Its\n",
      "leave-one-out variant is known to produce an estimator of the generalization\n",
      "error which is almost unbiased. Its major drawback rests in its time\n",
      "requirement. To overcome this difficulty, several upper bounds on the\n",
      "leave-one-out error of the pattern recognition SVM have been derived. Among\n",
      "those bounds, the most popular one is probably the radius-margin bound. It\n",
      "applies to the hard margin pattern recognition SVM, and by extension to the\n",
      "2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2,\n",
      "as a direct extension of the 2-norm SVM to the multi-class case. For this\n",
      "machine, a generalized radius-margin bound is then established.\n",
      "\n",
      "Cleaned Summary: Using a support vector machine requires to set two types of hyperparameters:\n",
      "the soft margin parameter C and the parameters of the kernel. To perform this\n",
      "model selection task, the method of choice is cross-validation. Its\n",
      "leave-one-out variant is known to produce an estimator of the generalization\n",
      "error which is almost unbiased. Its major drawback rests in its time\n",
      "requirement. To overcome this difficulty, several upper bounds on the\n",
      "leave-one-out error of the pattern recognition SVM have been derived. Among\n",
      "those bounds, the most popular one is probably the radius-margin bound. It\n",
      "applies to the hard margin pattern recognition SVM, and by extension to the\n",
      "2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2,\n",
      "as a direct extension of the 2-norm SVM to the multi-class case. For this\n",
      "machine, a generalized radius-margin bound is then established.\n",
      "\n",
      "Filtered Paper 8: Graph Kernels\n",
      "\n",
      "Link: http://arxiv.org/abs/0807.0093v1\n",
      "\n",
      "Original Summary: We present a unified framework to study graph kernels, special cases of which\n",
      "include the random walk graph kernel \\citep{GaeFlaWro03,BorOngSchVisetal05},\n",
      "marginalized graph kernel \\citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04},\n",
      "and geometric kernel on graphs \\citep{Gaertner02}. Through extensions of linear\n",
      "algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a\n",
      "Sylvester equation, we construct an algorithm that improves the time complexity\n",
      "of kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse,\n",
      "conjugate gradient solvers or fixed-point iterations bring our algorithm into\n",
      "the sub-cubic domain. Experiments on graphs from bioinformatics and other\n",
      "application domains show that it is often more than a thousand times faster\n",
      "than previous approaches. We then explore connections between diffusion kernels\n",
      "\\citep{KonLaf02}, regularization on graphs \\citep{SmoKon03}, and graph kernels,\n",
      "and use these connections to propose new graph kernels. Finally, we show that\n",
      "rational kernels \\citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized\n",
      "to graphs reduce to the random walk graph kernel.\n",
      "\n",
      "Cleaned Summary: We present a unified framework to study graph kernels, special cases of which\n",
      "include the random walk graph kernel ,\n",
      "marginalized graph kernel ,\n",
      "and geometric kernel on graphs . Through extensions of linear\n",
      "algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a\n",
      "Sylvester equation, we construct an algorithm that improves the time complexity\n",
      "of kernel computation from  to . When the graphs are sparse,\n",
      "conjugate gradient solvers or fixed-point iterations bring our algorithm into\n",
      "the sub-cubic domain. Experiments on graphs from bioinformatics and other\n",
      "application domains show that it is often more than a thousand times faster\n",
      "than previous approaches. We then explore connections between diffusion kernels\n",
      ", regularization on graphs , and graph kernels,\n",
      "and use these connections to propose new graph kernels. Finally, we show that\n",
      "rational kernels  when specialized\n",
      "to graphs reduce to the random walk graph kernel.\n",
      "\n",
      "Filtered Paper 9: When is there a representer theorem? Vector versus matrix regularizers\n",
      "\n",
      "Link: http://arxiv.org/abs/0809.1590v1\n",
      "\n",
      "Original Summary: We consider a general class of regularization methods which learn a vector of\n",
      "parameters on the basis of linear measurements. It is well known that if the\n",
      "regularizer is a nondecreasing function of the inner product then the learned\n",
      "vector is a linear combination of the input data. This result, known as the\n",
      "{\\em representer theorem}, is at the basis of kernel-based methods in machine\n",
      "learning. In this paper, we prove the necessity of the above condition, thereby\n",
      "completing the characterization of kernel methods based on regularization. We\n",
      "further extend our analysis to regularization methods which learn a matrix, a\n",
      "problem which is motivated by the application to multi-task learning. In this\n",
      "context, we study a more general representer theorem, which holds for a larger\n",
      "class of regularizers. We provide a necessary and sufficient condition for\n",
      "these class of matrix regularizers and highlight them with some concrete\n",
      "examples of practical importance. Our analysis uses basic principles from\n",
      "matrix theory, especially the useful notion of matrix nondecreasing function.\n",
      "\n",
      "Cleaned Summary: We consider a general class of regularization methods which learn a vector of\n",
      "parameters on the basis of linear measurements. It is well known that if the\n",
      "regularizer is a nondecreasing function of the inner product then the learned\n",
      "vector is a linear combination of the input data. This result, known as the\n",
      "representer theorem, is at the basis of kernel-based methods in machine\n",
      "learning. In this paper, we prove the necessity of the above condition, thereby\n",
      "completing the characterization of kernel methods based on regularization. We\n",
      "further extend our analysis to regularization methods which learn a matrix, a\n",
      "problem which is motivated by the application to multi-task learning. In this\n",
      "context, we study a more general representer theorem, which holds for a larger\n",
      "class of regularizers. We provide a necessary and sufficient condition for\n",
      "these class of matrix regularizers and highlight them with some concrete\n",
      "examples of practical importance. Our analysis uses basic principles from\n",
      "matrix theory, especially the useful notion of matrix nondecreasing function.\n",
      "\n",
      "Filtered Paper 10: Stability Bound for Stationary Phi-mixing and Beta-mixing Processes\n",
      "\n",
      "Link: http://arxiv.org/abs/0811.1629v1\n",
      "\n",
      "Original Summary: Most generalization bounds in learning theory are based on some measure of\n",
      "the complexity of the hypothesis class used, independently of any algorithm. In\n",
      "contrast, the notion of algorithmic stability can be used to derive tight\n",
      "generalization bounds that are tailored to specific learning algorithms by\n",
      "exploiting their particular properties. However, as in much of learning theory,\n",
      "existing stability analyses and bounds apply only in the scenario where the\n",
      "samples are independently and identically distributed. In many machine learning\n",
      "applications, however, this assumption does not hold. The observations received\n",
      "by the learning algorithm often have some inherent temporal dependence.\n",
      "  This paper studies the scenario where the observations are drawn from a\n",
      "stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in\n",
      "the study of non-i.i.d. processes that implies a dependence between\n",
      "observations weakening over time. We prove novel and distinct stability-based\n",
      "generalization bounds for stationary phi-mixing and beta-mixing sequences.\n",
      "These bounds strictly generalize the bounds given in the i.i.d. case and apply\n",
      "to all stable learning algorithms, thereby extending the use of\n",
      "stability-bounds to non-i.i.d. scenarios.\n",
      "  We also illustrate the application of our phi-mixing generalization bounds to\n",
      "general classes of learning algorithms, including Support Vector Regression,\n",
      "Kernel Ridge Regression, and Support Vector Machines, and many other kernel\n",
      "regularization-based and relative entropy-based regularization algorithms.\n",
      "These novel bounds can thus be viewed as the first theoretical basis for the\n",
      "use of these algorithms in non-i.i.d. scenarios.\n",
      "\n",
      "Cleaned Summary: Most generalization bounds in learning theory are based on some measure of\n",
      "the complexity of the hypothesis class used, independently of any algorithm. In\n",
      "contrast, the notion of algorithmic stability can be used to derive tight\n",
      "generalization bounds that are tailored to specific learning algorithms by\n",
      "exploiting their particular properties. However, as in much of learning theory,\n",
      "existing stability analyses and bounds apply only in the scenario where the\n",
      "samples are independently and identically distributed. In many machine learning\n",
      "applications, however, this assumption does not hold. The observations received\n",
      "by the learning algorithm often have some inherent temporal dependence.\n",
      "  This paper studies the scenario where the observations are drawn from a\n",
      "stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in\n",
      "the study of non-i.i.d. processes that implies a dependence between\n",
      "observations weakening over time. We prove novel and distinct stability-based\n",
      "generalization bounds for stationary phi-mixing and beta-mixing sequences.\n",
      "These bounds strictly generalize the bounds given in the i.i.d. case and apply\n",
      "to all stable learning algorithms, thereby extending the use of\n",
      "stability-bounds to non-i.i.d. scenarios.\n",
      "  We also illustrate the application of our phi-mixing generalization bounds to\n",
      "general classes of learning algorithms, including Support Vector Regression,\n",
      "Kernel Ridge Regression, and Support Vector Machines, and many other kernel\n",
      "regularization-based and relative entropy-based regularization algorithms.\n",
      "These novel bounds can thus be viewed as the first theoretical basis for the\n",
      "use of these algorithms in non-i.i.d. scenarios.\n",
      "\n",
      "Filtered Paper 11: Stability Analysis and Learning Bounds for Transductive Regression\n",
      "  Algorithms\n",
      "\n",
      "Link: http://arxiv.org/abs/0904.0814v1\n",
      "\n",
      "Original Summary: This paper uses the notion of algorithmic stability to derive novel\n",
      "generalization bounds for several families of transductive regression\n",
      "algorithms, both by using convexity and closed-form solutions. Our analysis\n",
      "helps compare the stability of these algorithms. It also shows that a number of\n",
      "widely used transductive regression algorithms are in fact unstable. Finally,\n",
      "it reports the results of experiments with local transductive regression\n",
      "demonstrating the benefit of our stability bounds for model selection, for one\n",
      "of the algorithms, in particular for determining the radius of the local\n",
      "neighborhood used by the algorithm.\n",
      "\n",
      "Cleaned Summary: This paper uses the notion of algorithmic stability to derive novel\n",
      "generalization bounds for several families of transductive regression\n",
      "algorithms, both by using convexity and closed-form solutions. Our analysis\n",
      "helps compare the stability of these algorithms. It also shows that a number of\n",
      "widely used transductive regression algorithms are in fact unstable. Finally,\n",
      "it reports the results of experiments with local transductive regression\n",
      "demonstrating the benefit of our stability bounds for model selection, for one\n",
      "of the algorithms, in particular for determining the radius of the local\n",
      "neighborhood used by the algorithm.\n",
      "\n",
      "Filtered Paper 12: A Bayesian Model for Supervised Clustering with the Dirichlet Process\n",
      "  Prior\n",
      "\n",
      "Link: http://arxiv.org/abs/0907.0808v1\n",
      "\n",
      "Original Summary: We develop a Bayesian framework for tackling the supervised clustering\n",
      "problem, the generic problem encountered in tasks such as reference matching,\n",
      "coreference resolution, identity uncertainty and record linkage. Our clustering\n",
      "model is based on the Dirichlet process prior, which enables us to define\n",
      "distributions over the countably infinite sets that naturally arise in this\n",
      "problem. We add supervision to our model by positing the existence of a set of\n",
      "unobserved random variables (we call these \"reference types\") that are generic\n",
      "across all clusters. Inference in our framework, which requires integrating\n",
      "over infinitely many parameters, is solved using Markov chain Monte Carlo\n",
      "techniques. We present algorithms for both conjugate and non-conjugate priors.\n",
      "We present a simple--but general--parameterization of our model based on a\n",
      "Gaussian assumption. We evaluate this model on one artificial task and three\n",
      "real-world tasks, comparing it against both unsupervised and state-of-the-art\n",
      "supervised algorithms. Our results show that our model is able to outperform\n",
      "other models across a variety of tasks and performance metrics.\n",
      "\n",
      "Cleaned Summary: We develop a Bayesian framework for tackling the supervised clustering\n",
      "problem, the generic problem encountered in tasks such as reference matching,\n",
      "coreference resolution, identity uncertainty and record linkage. Our clustering\n",
      "model is based on the Dirichlet process prior, which enables us to define\n",
      "distributions over the countably infinite sets that naturally arise in this\n",
      "problem. We add supervision to our model by positing the existence of a set of\n",
      "unobserved random variables (we call these \"reference types\") that are generic\n",
      "across all clusters. Inference in our framework, which requires integrating\n",
      "over infinitely many parameters, is solved using Markov chain Monte Carlo\n",
      "techniques. We present algorithms for both conjugate and non-conjugate priors.\n",
      "We present a simple--but general--parameterization of our model based on a\n",
      "Gaussian assumption. We evaluate this model on one artificial task and three\n",
      "real-world tasks, comparing it against both unsupervised and state-of-the-art\n",
      "supervised algorithms. Our results show that our model is able to outperform\n",
      "other models across a variety of tasks and performance metrics.\n",
      "\n",
      "Filtered Paper 13: Collaborative Filtering in a Non-Uniform World: Learning with the\n",
      "  Weighted Trace Norm\n",
      "\n",
      "Link: http://arxiv.org/abs/1002.2780v1\n",
      "\n",
      "Original Summary: We show that matrix completion with trace-norm regularization can be\n",
      "significantly hurt when entries of the matrix are sampled non-uniformly. We\n",
      "introduce a weighted version of the trace-norm regularizer that works well also\n",
      "with non-uniform sampling. Our experimental results demonstrate that the\n",
      "weighted trace-norm regularization indeed yields significant gains on the\n",
      "(highly non-uniformly sampled) Netflix dataset.\n",
      "\n",
      "Cleaned Summary: We show that matrix completion with trace-norm regularization can be\n",
      "significantly hurt when entries of the matrix are sampled non-uniformly. We\n",
      "introduce a weighted version of the trace-norm regularizer that works well also\n",
      "with non-uniform sampling. Our experimental results demonstrate that the\n",
      "weighted trace-norm regularization indeed yields significant gains on the\n",
      "(highly non-uniformly sampled) Netflix dataset.\n",
      "\n",
      "Filtered Paper 14: Adaptive Bound Optimization for Online Convex Optimization\n",
      "\n",
      "Link: http://arxiv.org/abs/1002.4908v2\n",
      "\n",
      "Original Summary: We introduce a new online convex optimization algorithm that adaptively\n",
      "chooses its regularization function based on the loss functions observed so\n",
      "far. This is in contrast to previous algorithms that use a fixed regularization\n",
      "function such as L2-squared, and modify it only via a single time-dependent\n",
      "parameter. Our algorithm's regret bounds are worst-case optimal, and for\n",
      "certain realistic classes of loss functions they are much better than existing\n",
      "bounds. These bounds are problem-dependent, which means they can exploit the\n",
      "structure of the actual problem instance. Critically, however, our algorithm\n",
      "does not need to know this structure in advance. Rather, we prove competitive\n",
      "guarantees that show the algorithm provides a bound within a constant factor of\n",
      "the best possible bound (of a certain functional form) in hindsight.\n",
      "\n",
      "Cleaned Summary: We introduce a new online convex optimization algorithm that adaptively\n",
      "chooses its regularization function based on the loss functions observed so\n",
      "far. This is in contrast to previous algorithms that use a fixed regularization\n",
      "function such as L2-squared, and modify it only via a single time-dependent\n",
      "parameter. Our algorithm's regret bounds are worst-case optimal, and for\n",
      "certain realistic classes of loss functions they are much better than existing\n",
      "bounds. These bounds are problem-dependent, which means they can exploit the\n",
      "structure of the actual problem instance. Critically, however, our algorithm\n",
      "does not need to know this structure in advance. Rather, we prove competitive\n",
      "guarantees that show the algorithm provides a bound within a constant factor of\n",
      "the best possible bound (of a certain functional form) in hindsight.\n",
      "\n",
      "Filtered Paper 15: Model Selection with the Loss Rank Principle\n",
      "\n",
      "Link: http://arxiv.org/abs/1003.0516v1\n",
      "\n",
      "Original Summary: A key issue in statistics and machine learning is to automatically select the\n",
      "\"right\" model complexity, e.g., the number of neighbors to be averaged over in\n",
      "k nearest neighbor (kNN) regression or the polynomial degree in regression with\n",
      "polynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) -\n",
      "for model selection in regression and classification. It is based on the loss\n",
      "rank, which counts how many other (fictitious) data would be fitted better.\n",
      "LoRP selects the model that has minimal loss rank. Unlike most penalized\n",
      "maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the\n",
      "regression functions and the loss function. It works without a stochastic noise\n",
      "model, and is directly applicable to any non-parametric regressor, like kNN.\n",
      "\n",
      "Cleaned Summary: A key issue in statistics and machine learning is to automatically select the\n",
      "\"right\" model complexity, e.g., the number of neighbors to be averaged over in\n",
      "k nearest neighbor (kNN) regression or the polynomial degree in regression with\n",
      "polynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) -\n",
      "for model selection in regression and classification. It is based on the loss\n",
      "rank, which counts how many other (fictitious) data would be fitted better.\n",
      "LoRP selects the model that has minimal loss rank. Unlike most penalized\n",
      "maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the\n",
      "regression functions and the loss function. It works without a stochastic noise\n",
      "model, and is directly applicable to any non-parametric regressor, like kNN.\n",
      "\n",
      "Filtered Paper 16: Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable\n",
      "  Information Criterion in Singular Learning Theory\n",
      "\n",
      "Link: http://arxiv.org/abs/1004.2316v2\n",
      "\n",
      "Original Summary: In regular statistical models, the leave-one-out cross-validation is\n",
      "asymptotically equivalent to the Akaike information criterion. However, since\n",
      "many learning machines are singular statistical models, the asymptotic behavior\n",
      "of the cross-validation remains unknown. In previous studies, we established\n",
      "the singular learning theory and proposed a widely applicable information\n",
      "criterion, the expectation value of which is asymptotically equal to the\n",
      "average Bayes generalization loss. In the present paper, we theoretically\n",
      "compare the Bayes cross-validation loss and the widely applicable information\n",
      "criterion and prove two theorems. First, the Bayes cross-validation loss is\n",
      "asymptotically equivalent to the widely applicable information criterion as a\n",
      "random variable. Therefore, model selection and hyperparameter optimization\n",
      "using these two values are asymptotically equivalent. Second, the sum of the\n",
      "Bayes generalization error and the Bayes cross-validation error is\n",
      "asymptotically equal to $2\\lambda/n$, where $\\lambda$ is the real log canonical\n",
      "threshold and $n$ is the number of training samples. Therefore the relation\n",
      "between the cross-validation error and the generalization error is determined\n",
      "by the algebraic geometrical structure of a learning machine. We also clarify\n",
      "that the deviance information criteria are different from the Bayes\n",
      "cross-validation and the widely applicable information criterion.\n",
      "\n",
      "Cleaned Summary: In regular statistical models, the leave-one-out cross-validation is\n",
      "asymptotically equivalent to the Akaike information criterion. However, since\n",
      "many learning machines are singular statistical models, the asymptotic behavior\n",
      "of the cross-validation remains unknown. In previous studies, we established\n",
      "the singular learning theory and proposed a widely applicable information\n",
      "criterion, the expectation value of which is asymptotically equal to the\n",
      "average Bayes generalization loss. In the present paper, we theoretically\n",
      "compare the Bayes cross-validation loss and the widely applicable information\n",
      "criterion and prove two theorems. First, the Bayes cross-validation loss is\n",
      "asymptotically equivalent to the widely applicable information criterion as a\n",
      "random variable. Therefore, model selection and hyperparameter optimization\n",
      "using these two values are asymptotically equivalent. Second, the sum of the\n",
      "Bayes generalization error and the Bayes cross-validation error is\n",
      "asymptotically equal to , where  is the real log canonical\n",
      "threshold and  is the number of training samples. Therefore the relation\n",
      "between the cross-validation error and the generalization error is determined\n",
      "by the algebraic geometrical structure of a learning machine. We also clarify\n",
      "that the deviance information criteria are different from the Bayes\n",
      "cross-validation and the widely applicable information criterion.\n",
      "\n",
      "Filtered Paper 17: Filtrage vaste marge pour l'étiquetage séquentiel à noyaux de\n",
      "  signaux\n",
      "\n",
      "Link: http://arxiv.org/abs/1007.0824v1\n",
      "\n",
      "Original Summary: We address in this paper the problem of multi-channel signal sequence\n",
      "labeling. In particular, we consider the problem where the signals are\n",
      "contaminated by noise or may present some dephasing with respect to their\n",
      "labels. For that, we propose to jointly learn a SVM sample classifier with a\n",
      "temporal filtering of the channels. This will lead to a large margin filtering\n",
      "that is adapted to the specificity of each channel (noise and time-lag). We\n",
      "derive algorithms to solve the optimization problem and we discuss different\n",
      "filter regularizations for automated scaling or selection of channels. Our\n",
      "approach is tested on a non-linear toy example and on a BCI dataset. Results\n",
      "show that the classification performance on these problems can be improved by\n",
      "learning a large margin filtering.\n",
      "\n",
      "Cleaned Summary: We address in this paper the problem of multi-channel signal sequence\n",
      "labeling. In particular, we consider the problem where the signals are\n",
      "contaminated by noise or may present some dephasing with respect to their\n",
      "labels. For that, we propose to jointly learn a SVM sample classifier with a\n",
      "temporal filtering of the channels. This will lead to a large margin filtering\n",
      "that is adapted to the specificity of each channel (noise and time-lag). We\n",
      "derive algorithms to solve the optimization problem and we discuss different\n",
      "filter regularizations for automated scaling or selection of channels. Our\n",
      "approach is tested on a non-linear toy example and on a BCI dataset. Results\n",
      "show that the classification performance on these problems can be improved by\n",
      "learning a large margin filtering.\n",
      "\n",
      "Filtered Paper 18: A Unified View of Regularized Dual Averaging and Mirror Descent with\n",
      "  Implicit Updates\n",
      "\n",
      "Link: http://arxiv.org/abs/1009.3240v2\n",
      "\n",
      "Original Summary: We study three families of online convex optimization algorithms:\n",
      "follow-the-proximally-regularized-leader (FTRL-Proximal), regularized dual\n",
      "averaging (RDA), and composite-objective mirror descent. We first prove\n",
      "equivalence theorems that show all of these algorithms are instantiations of a\n",
      "general FTRL update. This provides theoretical insight on previous experimental\n",
      "observations. In particular, even though the FOBOS composite mirror descent\n",
      "algorithm handles L1 regularization explicitly, it has been observed that RDA\n",
      "is even more effective at producing sparsity. Our results demonstrate that\n",
      "FOBOS uses subgradient approximations to the L1 penalty from previous rounds,\n",
      "leading to less sparsity than RDA, which handles the cumulative penalty in\n",
      "closed form. The FTRL-Proximal algorithm can be seen as a hybrid of these two,\n",
      "and outperforms both on a large, real-world dataset.\n",
      "  Our second contribution is a unified analysis which produces regret bounds\n",
      "that match (up to logarithmic terms) or improve the best previously known\n",
      "bounds. This analysis also extends these algorithms in two important ways: we\n",
      "support a more general type of composite objective and we analyze implicit\n",
      "updates, which replace the subgradient approximation of the current loss\n",
      "function with an exact optimization.\n",
      "\n",
      "Cleaned Summary: We study three families of online convex optimization algorithms:\n",
      "follow-the-proximally-regularized-leader (FTRL-Proximal), regularized dual\n",
      "averaging (RDA), and composite-objective mirror descent. We first prove\n",
      "equivalence theorems that show all of these algorithms are instantiations of a\n",
      "general FTRL update. This provides theoretical insight on previous experimental\n",
      "observations. In particular, even though the FOBOS composite mirror descent\n",
      "algorithm handles L1 regularization explicitly, it has been observed that RDA\n",
      "is even more effective at producing sparsity. Our results demonstrate that\n",
      "FOBOS uses subgradient approximations to the L1 penalty from previous rounds,\n",
      "leading to less sparsity than RDA, which handles the cumulative penalty in\n",
      "closed form. The FTRL-Proximal algorithm can be seen as a hybrid of these two,\n",
      "and outperforms both on a large, real-world dataset.\n",
      "  Our second contribution is a unified analysis which produces regret bounds\n",
      "that match (up to logarithmic terms) or improve the best previously known\n",
      "bounds. This analysis also extends these algorithms in two important ways: we\n",
      "support a more general type of composite objective and we analyze implicit\n",
      "updates, which replace the subgradient approximation of the current loss\n",
      "function with an exact optimization.\n",
      "\n",
      "Filtered Paper 19: Efficient L1/Lq Norm Regularization\n",
      "\n",
      "Link: http://arxiv.org/abs/1009.4766v1\n",
      "\n",
      "Original Summary: Sparse learning has recently received increasing attention in many areas\n",
      "including machine learning, statistics, and applied mathematics. The mixed-norm\n",
      "regularization based on the L1/Lq norm with q > 1 is attractive in many\n",
      "applications of regression and classification in that it facilitates group\n",
      "sparsity in the model. The resulting optimization problem is, however,\n",
      "challenging to solve due to the structure of the L1/Lq -regularization.\n",
      "Existing work deals with special cases including q = 2,infinity, and they\n",
      "cannot be easily extended to the general case. In this paper, we propose an\n",
      "efficient algorithm based on the accelerated gradient method for solving the\n",
      "L1/Lq -regularized problem, which is applicable for all values of q larger than\n",
      "1, thus significantly extending existing work. One key building block of the\n",
      "proposed algorithm is the L1/Lq -regularized Euclidean projection (EP1q). Our\n",
      "theoretical analysis reveals the key properties of EP1q and illustrates why\n",
      "EP1q for the general q is significantly more challenging to solve than the\n",
      "special cases. Based on our theoretical analysis, we develop an efficient\n",
      "algorithm for EP1q by solving two zero finding problems. Experimental results\n",
      "demonstrate the efficiency of the proposed algorithm.\n",
      "\n",
      "Cleaned Summary: Sparse learning has recently received increasing attention in many areas\n",
      "including machine learning, statistics, and applied mathematics. The mixed-norm\n",
      "regularization based on the L1/Lq norm with q > 1 is attractive in many\n",
      "applications of regression and classification in that it facilitates group\n",
      "sparsity in the model. The resulting optimization problem is, however,\n",
      "challenging to solve due to the structure of the L1/Lq -regularization.\n",
      "Existing work deals with special cases including q = 2,infinity, and they\n",
      "cannot be easily extended to the general case. In this paper, we propose an\n",
      "efficient algorithm based on the accelerated gradient method for solving the\n",
      "L1/Lq -regularized problem, which is applicable for all values of q larger than\n",
      "1, thus significantly extending existing work. One key building block of the\n",
      "proposed algorithm is the L1/Lq -regularized Euclidean projection (EP1q). Our\n",
      "theoretical analysis reveals the key properties of EP1q and illustrates why\n",
      "EP1q for the general q is significantly more challenging to solve than the\n",
      "special cases. Based on our theoretical analysis, we develop an efficient\n",
      "algorithm for EP1q by solving two zero finding problems. Experimental results\n",
      "demonstrate the efficiency of the proposed algorithm.\n",
      "\n",
      "Filtered Paper 20: Multi-parametric Solution-path Algorithm for Instance-weighted Support\n",
      "  Vector Machines\n",
      "\n",
      "Link: http://arxiv.org/abs/1009.4791v2\n",
      "\n",
      "Original Summary: An instance-weighted variant of the support vector machine (SVM) has\n",
      "attracted considerable attention recently since they are useful in various\n",
      "machine learning tasks such as non-stationary data analysis, heteroscedastic\n",
      "data modeling, transfer learning, learning to rank, and transduction. An\n",
      "important challenge in these scenarios is to overcome the computational\n",
      "bottleneck---instance weights often change dynamically or adaptively, and thus\n",
      "the weighted SVM solutions must be repeatedly computed. In this paper, we\n",
      "develop an algorithm that can efficiently and exactly update the weighted SVM\n",
      "solutions for arbitrary change of instance weights. Technically, this\n",
      "contribution can be regarded as an extension of the conventional solution-path\n",
      "algorithm for a single regularization parameter to multiple instance-weight\n",
      "parameters. However, this extension gives rise to a significant problem that\n",
      "breakpoints (at which the solution path turns) have to be identified in\n",
      "high-dimensional space. To facilitate this, we introduce a parametric\n",
      "representation of instance weights. We also provide a geometric interpretation\n",
      "in weight space using a notion of critical region: a polyhedron in which the\n",
      "current affine solution remains to be optimal. Then we find breakpoints at\n",
      "intersections of the solution path and boundaries of polyhedrons. Through\n",
      "extensive experiments on various practical applications, we demonstrate the\n",
      "usefulness of the proposed algorithm.\n",
      "\n",
      "Cleaned Summary: An instance-weighted variant of the support vector machine (SVM) has\n",
      "attracted considerable attention recently since they are useful in various\n",
      "machine learning tasks such as non-stationary data analysis, heteroscedastic\n",
      "data modeling, transfer learning, learning to rank, and transduction. An\n",
      "important challenge in these scenarios is to overcome the computational\n",
      "bottleneck---instance weights often change dynamically or adaptively, and thus\n",
      "the weighted SVM solutions must be repeatedly computed. In this paper, we\n",
      "develop an algorithm that can efficiently and exactly update the weighted SVM\n",
      "solutions for arbitrary change of instance weights. Technically, this\n",
      "contribution can be regarded as an extension of the conventional solution-path\n",
      "algorithm for a single regularization parameter to multiple instance-weight\n",
      "parameters. However, this extension gives rise to a significant problem that\n",
      "breakpoints (at which the solution path turns) have to be identified in\n",
      "high-dimensional space. To facilitate this, we introduce a parametric\n",
      "representation of instance weights. We also provide a geometric interpretation\n",
      "in weight space using a notion of critical region: a polyhedron in which the\n",
      "current affine solution remains to be optimal. Then we find breakpoints at\n",
      "intersections of the solution path and boundaries of polyhedrons. Through\n",
      "extensive experiments on various practical applications, we demonstrate the\n",
      "usefulness of the proposed algorithm.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display filtered papers\n",
    "\n",
    "for idx, paper in enumerate(filtered_papers, 1):\n",
    "    print(f\"Filtered Paper {idx}: {paper['title']}\\n\")\n",
    "    print(f\"Link: {paper['link']}\\n\")\n",
    "    print(f\"Original Summary: {paper['summary']}\\n\")\n",
    "    print(f\"Cleaned Summary: {paper['cleaned_summary']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying Filtered and Cleaned Papers\n",
    "\n",
    "This step prints the filtered and cleaned papers. For each paper, we display:\n",
    "- The title\n",
    "- The original summary\n",
    "- The cleaned summary\n",
    "- A link to the paper\n",
    "\n",
    "This allows us to quickly verify that we have the correct papers and summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(papers, filename=\"filtered_papers.csv\"):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        fieldnames = ['title', 'link', 'cleaned_summary']\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for paper in papers:\n",
    "            writer.writerow({'title': paper['title'], 'link': paper['link'], 'cleaned_summary': paper['cleaned_summary']})\n",
    "\n",
    "save_to_csv(filtered_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Filtered Papers to a CSV File\n",
    "\n",
    "After filtering and cleaning the papers, we save the results into a CSV file named `filtered_papers.csv`. Each row contains:\n",
    "- The title of the paper.\n",
    "- The link to the paper.\n",
    "- The cleaned summary.\n",
    "\n",
    "This ensures that the results can be easily accessed and analyzed later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of filtered papers: 20\n"
     ]
    }
   ],
   "source": [
    "def count_papers(filename=\"filtered_papers.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    return len(df)\n",
    "\n",
    "total_papers = count_papers()\n",
    "print(f\"Total number of filtered papers: {total_papers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting the Number of Papers in the CSV\n",
    "\n",
    "In this final step, we read the CSV file to count how many papers have been saved. This provides a quick check to ensure that the correct number of papers (20) has been fetched and saved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
