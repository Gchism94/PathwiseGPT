{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WwvSJF2-IJxf"
      },
      "outputs": [],
      "source": [
        "import os # Handles file paths and directories\n",
        "import json # Parses and loads JSON files for rules and configurations\n",
        "import unittest # Framework for writing and running unit tests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To load textual data rules from JSON file\n",
        "with open(\"analysis/data/derivedData/rules_textual.json\", \"r\") as file:\n",
        "    textual_data_rules = json.load(file)"
      ],
      "metadata": {
        "id": "shsW6A8rx_tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To load configuration file for dataset thresholds\n",
        "with open(\"analysis/data/derivedData/config.json\", \"r\") as config_file:\n",
        "    config = json.load(config_file)\n",
        "\n",
        "# Threshold values for small and large datasets\n",
        "SMALL_THRESHOLD = config[\"dataset_thresholds\"][\"small_dataset\"]\n",
        "LARGE_THRESHOLD = config[\"dataset_thresholds\"][\"large_dataset\"]"
      ],
      "metadata": {
        "id": "0rezPjUsVg8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_category(dataset_size):\n",
        "    \"\"\"\n",
        "    Determines the dataset category based on its size.\n",
        "\n",
        "    Args:\n",
        "        dataset_size (int): The size of the dataset.\n",
        "\n",
        "    Returns:\n",
        "        str: Dataset category ('small_dataset', 'medium_dataset', or 'large_dataset').\n",
        "    \"\"\"\n",
        "    if dataset_size < SMALL_THRESHOLD:\n",
        "        return \"small_dataset\"\n",
        "    elif dataset_size > LARGE_THRESHOLD:\n",
        "        return \"large_dataset\"\n",
        "    else:\n",
        "        return \"medium_dataset\"\n"
      ],
      "metadata": {
        "id": "W8ZVDClnYJHT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_logic(models, condition=None, dataset_size=None):\n",
        "    \"\"\"\n",
        "    Validates if the models fit the task-specific conditions.\n",
        "\n",
        "    Args:\n",
        "        models (list): List of models retrieved from the rules.\n",
        "        condition (str, optional): Specific condition being validated.\n",
        "        dataset_size (int, optional): Dataset size for validation.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if validation passes, False otherwise.\n",
        "    \"\"\"\n",
        "    if not models:\n",
        "        print(f\"Warning: No models fit the condition '{condition}' for dataset size '{dataset_size}'.\")\n",
        "        return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "HmdN8XyHs44q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_flat_conditions(sub_conditions, indent_level=2):\n",
        "    \"\"\"\n",
        "    Handles flat conditions for model selection.\n",
        "\n",
        "    Args:\n",
        "        sub_conditions (dict): Dictionary of model approaches and model lists.\n",
        "        indent_level (int): Indentation level for formatted output.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated logic for flat conditions.\n",
        "    \"\"\"\n",
        "    logic = \"\"\n",
        "    indent = \"  \" * indent_level\n",
        "    for approach, model_list in sub_conditions.items():\n",
        "        logic += f\"{indent}Use {approach} model: {', '.join(model_list)}\\n\"\n",
        "    return logic"
      ],
      "metadata": {
        "id": "f5qX2t9zN9w1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_nested_conditions(sub_conditions, indent_level=2):\n",
        "    \"\"\"\n",
        "    Recursively handles nested conditions.\n",
        "\n",
        "    Args:\n",
        "        sub_conditions (dict): Dictionary of nested conditions and models.\n",
        "        indent_level (int): Indentation level for formatting.\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted logic for nested conditions.\n",
        "    \"\"\"\n",
        "    logic = \"\"\n",
        "    indent = \"  \" * indent_level\n",
        "    for condition, models in sub_conditions.items():\n",
        "        if isinstance(models, dict):  # To handle nested conditions\n",
        "            logic += f\"{indent}If {condition}:\\n\"\n",
        "            logic += handle_nested_conditions(models, indent_level + 1)\n",
        "        else:  # To handle flat conditions\n",
        "            logic += handle_flat_conditions({condition: models}, indent_level)\n",
        "    return logic"
      ],
      "metadata": {
        "id": "M8wDvBCTN9zA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_task_logic(data, task_name, dataset_size=None):\n",
        "    \"\"\"\n",
        "    Dynamically generates logic for a given textual task.\n",
        "\n",
        "    Args:\n",
        "        data (dict): Textual data rules.\n",
        "        task_name (str): Task name (e.g., 'text_classification').\n",
        "        dataset_size (int, optional): Dataset size for validation.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated logic for the given task.\n",
        "    \"\"\"\n",
        "    logic_text = f\"If the problem is {task_name.replace('_', ' ')}:\\n\"\n",
        "    task_details = data[\"tasks\"][task_name]\n",
        "\n",
        "    for condition, sub_conditions in task_details.items():\n",
        "        if condition == \"default\":\n",
        "            logic_text += \"  Else:\\n\"\n",
        "        else:\n",
        "            logic_text += f\"  If {condition}:\\n\"\n",
        "\n",
        "        # Validate and generate logic based on conditions\n",
        "        if isinstance(sub_conditions, dict):\n",
        "            if isinstance(next(iter(sub_conditions.values())), dict):  # Nested structure\n",
        "                logic_text += handle_nested_conditions(sub_conditions)\n",
        "            else: # Flat conditions\n",
        "                logic_text += handle_flat_conditions(sub_conditions, indent_level=2)\n",
        "        else:\n",
        "            logic_text += handle_flat_conditions(sub_conditions, indent_level=2)\n",
        "\n",
        "    return logic_text"
      ],
      "metadata": {
        "id": "D4UIw9ytM__j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_textual_logic(data_type, task, dataset_size=None):\n",
        "    \"\"\"\n",
        "    Generates ML/DL decision logic for textual tasks.\n",
        "\n",
        "    Args:\n",
        "        data_type (str): Type of data (e.g., 'Textual').\n",
        "        task (str): Task type (e.g., 'text_classification').\n",
        "        dataset_size (int, optional): Dataset size for validation.\n",
        "\n",
        "    Returns:\n",
        "        str: Decision logic text for the specified task.\n",
        "    \"\"\"\n",
        "    # Validate if the task exists in the rules\n",
        "    valid_tasks = textual_data_rules[\"tasks\"].keys()\n",
        "    if task not in valid_tasks:\n",
        "        raise ValueError(f\"Task '{task}' is not supported. Available tasks: {', '.join(valid_tasks)}\")\n",
        "\n",
        "    return generate_task_logic(textual_data_rules, task, dataset_size)"
      ],
      "metadata": {
        "id": "tXdMkQRFNADM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_textual_logic(\"Textual\", \"text_classification\", dataset_size=500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JafV8kQpIp9W",
        "outputId": "46a6830f-6fb9-4abe-cce7-ef2df7156815"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the problem is text classification:\n",
            "  If independent_features:\n",
            "    Use ML model: Naive Bayes\n",
            "  If small_dataset:\n",
            "    Use DL model: Pre-trained BERT, DistilBERT\n",
            "  If high_dimensional:\n",
            "    Use ML model: Support Vector Machine (SVM)\n",
            "  If requires_interpretability:\n",
            "    Use DL model: Transformer-based Models (e.g., BERT, GPT)\n",
            "  Else:\n",
            "    Use DL model: Recurrent Neural Networks (RNN), LSTMs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_textual_logic(\"Textual\", \"language_modeling_and_generation\", dataset_size=10000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uaw9idpNAFP",
        "outputId": "31d34093-e3d2-4939-d2f5-fe03457ad09e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the problem is language modeling and generation:\n",
            "  If requires_low_latency:\n",
            "    Use DL model: DistilGPT\n",
            "  If requires_large_context_window:\n",
            "    Use DL model: GPT, GPT-2, GPT-3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_textual_logic(\"Textual\", \"text_summarization\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYgERhWXIqAJ",
        "outputId": "53fab665-0f02-4255-f1f9-e5eeed4fea39"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the problem is text summarization:\n",
            "  If extractive:\n",
            "    If small_dataset:\n",
            "      Use ML model: TextRank\n",
            "    If default:\n",
            "      Use DL model: BERTSum\n",
            "  If abstractive:\n",
            "    Use DL model: T5, GPT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_textual_logic(\"Textual\", \"named_entity_recognition\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp9mMY95JHU_",
        "outputId": "23242ee4-0cc1-466d-d9dc-2d409606ffdc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the problem is named entity recognition:\n",
            "  If fine_grained:\n",
            "    Use ML model: Conditional Random Fields (CRF)\n",
            "  If small_dataset:\n",
            "    Use DL model: Pre-trained BERT\n",
            "  Else:\n",
            "    Use DL model: Train Transformers from Scratch\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_textual_logic(\"Textual\", \"question_answering\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCCasa6GJTaA",
        "outputId": "7a408c8a-bb1b-4f73-80c7-f1daa217dd22"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the problem is question answering:\n",
            "  If requires_complex_reasoning:\n",
            "    Use DL model: BERT, RoBERTa\n",
            "  Else:\n",
            "    Use DL model: DistilBERT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_textual_logic(\"Textual\", \"machine_translation\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMqC7BnwJTcO",
        "outputId": "0161a646-770d-4bc6-bc7b-0b4c9bc5f493"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the problem is machine translation:\n",
            "  If requires_high_speed:\n",
            "    Use DL model: MarianMT\n",
            "  If requires_contextual_translation:\n",
            "    Use DL model: T5, OpenNMT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_textual_logic(\"Textual\", \"topic_modeling\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6VXHcTqNAI4",
        "outputId": "1885f8c9-c058-4aa7-aa53-014f43609ab8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the problem is topic modeling:\n",
            "  If sparse_topics:\n",
            "    Use ML model: Non-negative Matrix Factorization (NMF)\n",
            "  Else:\n",
            "    Use ML model: Latent Dirichlet Allocation (LDA)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_textual_logic(\"Textual\", \"text_to_speech\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2iJUV1BNAG_",
        "outputId": "21a40761-8308-41b7-beef-53ee0b2f0ccb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the problem is text to speech:\n",
            "  If requires_high_fidelity:\n",
            "    Use DL model: WaveNet\n",
            "  If requires_fast_processing:\n",
            "    Use DL model: Tacotron 2, FastSpeech\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_textual_logic(\"Textual\", \"speech_to_text\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZexq8j6Jrme",
        "outputId": "d27bfe93-1f25-4cc8-f55d-3a27b4f2e33f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the problem is speech to text:\n",
            "  If requires_high_accuracy:\n",
            "    Use DL model: DeepSpeech, Wav2Vec 2.0\n",
            "  Else:\n",
            "    Use DL model: Speech Transformers\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test scenarios for logic validation\n",
        "test_scenarios = [\n",
        "    {\n",
        "    \"data_type\": \"Textual\",\n",
        "    \"task\": \"named_entity_recognition\",\n",
        "    \"dataset_size\": 500,  # Small dataset\n",
        "    \"expected_model\": \"Conditional Random Fields (CRF)\"\n",
        "    },\n",
        "    {\n",
        "        \"data_type\": \"Textual\",\n",
        "        \"task\": \"text_summarization\",\n",
        "        \"dataset_size\": 10000,  # Large dataset\n",
        "        \"expected_model\": \"BERTSum\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "0dg4sPyDux1n"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestTextualLogic(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit tests to validate textual logic against JSON rules.\n",
        "    \"\"\"\n",
        "\n",
        "    def test_logic(self):\n",
        "        \"\"\"\n",
        "        Test logic dynamically across multiple scenarios.\n",
        "\n",
        "        This test iterates over predefined test cases to validate model mappings.\n",
        "        \"\"\"\n",
        "        for scenario in test_scenarios:\n",
        "            with self.subTest(scenario=scenario):\n",
        "                result = generate_textual_logic(\n",
        "                    data_type=scenario[\"data_type\"],\n",
        "                    task=scenario[\"task\"],\n",
        "                    dataset_size=scenario[\"dataset_size\"]\n",
        "                )\n",
        "                self.assertIn(\n",
        "                    scenario[\"expected_model\"],\n",
        "                    result,\n",
        "                    f\"Failed for {scenario['task']} with dataset size {scenario['dataset_size']}\"\n",
        "                )\n",
        "\n",
        "    def test_invalid_task(self):\n",
        "        \"\"\"\n",
        "        Test behavior when an invalid task is provided.\n",
        "\n",
        "        Ensures that unsupported tasks raise a ValueError.\n",
        "        \"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            generate_textual_logic(\"Textual\", \"invalid_task\")"
      ],
      "metadata": {
        "id": "4fo2eaGxux3z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=[''], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YoYv0TRux6V",
        "outputId": "8412b1e0-219d-403a-b66d-80508446ee53"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.004s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    }
  ]
}